{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import gym"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def value_iteration(env, gamma, max_k):\n",
    "    # Helper method:\n",
    "    # Action-value function. Returns the expected reward to get from\n",
    "    # the given state and taking the given action. V is a list of the optimal\n",
    "    # state values for the next time step\n",
    "    def action_value_function(state, action, V):\n",
    "        q_value = 0\n",
    "        for prob, next_state, reward, _ in env.P[state][action]:\n",
    "            q_value += prob * (reward + gamma * V[next_state])\n",
    "        return q_value\n",
    "\n",
    "    # Initialization:\n",
    "    # k=0. We have 0 time steps left, meaning that the game\n",
    "    # is finished and the agent cannot do anything\n",
    "    # so the value of every state is 0\n",
    "    V = np.zeros(env.nS)\n",
    "\n",
    "    # Now, having the optimal state values when there are k-1 time steps left\n",
    "    # we can calculate the optimal state values when there are k time steps left.\n",
    "    # This thanks to using a Dynamic Programming Bottom-up approach\n",
    "    for k in range(1, max_k):\n",
    "        # Keep a copy of V because we are going to be modifying V and we\n",
    "        # need to be taking a look at the values of the previous V\n",
    "        V_prev = np.copy(V)\n",
    "        \n",
    "        # Calculate the optimal value for each state when there\n",
    "        # are k time steps left\n",
    "        for state in range(env.nS):\n",
    "            # Calculate all the optimal Q values (action values) for the current state and time step\n",
    "            q_values = [action_value_function(state, action, V_prev) for action in range(env.nA)]\n",
    "            \n",
    "            # The value of the state in the current time step is the maximum of the Q values\n",
    "            V[state] = max(q_values)\n",
    "\n",
    "    return V"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 0.82352941  0.82352941  0.82352941  0.82352941  0.82352941  0.\n",
      "  0.52941176  0.          0.82352941  0.82352941  0.76470588  0.          0.\n",
      "  0.88235294  0.94117647  0.        ]\n"
     ]
    }
   ],
   "source": [
    "env = gym.make('FrozenLake-v0').unwrapped\n",
    "print(value_iteration(env, 1.0, 1000))\n",
    "env.close()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
