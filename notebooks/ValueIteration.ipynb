{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import gym"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def action_value_function(state, action, V, transition_matrix, gamma):\n",
    "    \"\"\"(Helper method)\n",
    "    Action-value function. Returns the expected reward to get from the given state and \n",
    "    taking the given action. \n",
    "    \n",
    "    Args:\n",
    "        state: current state.\n",
    "        action: action to take from the current state.\n",
    "        V: Vector of the optimal state values for the next time step.\n",
    "        transition_matrix: Transition matrix of the MDP, each cell [s][a] is a list of tuples \n",
    "            (probability, next state, reward, done). Meaning that in state s and taking\n",
    "            action a, we can get to any of those next states with the respective probability\n",
    "            and reward.\n",
    "        gamma: discount factor.\n",
    "        \n",
    "    Returns:\n",
    "        The q value (also known as action value) for the given state and action\n",
    "    \"\"\"\n",
    "    q_value = 0\n",
    "    for prob, next_state, reward, _ in transition_matrix[state][action]:\n",
    "        q_value += prob * (reward + gamma * V[next_state])\n",
    "    return q_value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def value_iteration(env, gamma, stop_callback):\n",
    "    \"\"\"Value iteration algorithm is used to find the optimal state values (or action values\n",
    "    but generally the state values are computed because they need less or the same memory) for \n",
    "    a Markov Decision Process where everything is known (the set of states, set of actions, reward \n",
    "    function and transition function).\n",
    "    \n",
    "    Value iteration uses a Dynamic Programming approach to find the optimal values because the\n",
    "    formula can be expressed in a recursive way and the subproblems overlap. Thanks to this we\n",
    "    have an exact method to calculate the optimal values in polynomial complexity. \n",
    "    \n",
    "    However, as many other problems, an exact method is very inefficient for very large problems.\n",
    "    This and having to know every parameter of the MDP are the weak points of the algorithms based\n",
    "    on Dynamic Programming for getting an optimal policy (or optimal values that we can use to get \n",
    "    the policy).\n",
    "    \n",
    "    Another thing to note is that this is not Reinforcement Learning, note that here we know everything\n",
    "    about the MDP and we are just applying an exact method to \"solve\" it, we are not learning anything.\n",
    "    \n",
    "    Args:\n",
    "        env: The Markov Decision Process with all parameters known.\n",
    "        gamma: The discount factor.\n",
    "        stop_callback: Callback to indicate when to stop. It receives the current\n",
    "            iteration index, a numpy vector of the previous state values and a numpy\n",
    "            vector of the current state values. It must return a boolean indicating whether\n",
    "            the algorithm should stop or not (return True to stop, False to continue). This\n",
    "            way we can stop the algorithm by a maximum number of iterations or when the difference\n",
    "            between the values from one iteration to the next is so small that we can consider\n",
    "            that the algorithm converged.\n",
    "\n",
    "    Returns:\n",
    "        A numpy array with the optimal state values when there are k time steps left\n",
    "        to finish the game.\n",
    "    \"\"\"\n",
    "    # Maximum number of iterations to do\n",
    "    # So the algorithm doesn't run forever\n",
    "    MAX_K = int(1E9)\n",
    "\n",
    "    # Initialization:\n",
    "    # k=0. We have 0 time steps left, meaning that the game\n",
    "    # is finished and the agent cannot do anything\n",
    "    # so the value of every state is 0\n",
    "    # Note that we are formulating the problem this way to use a Dynamic\n",
    "    # Programming tabulation approach\n",
    "    V = np.zeros(env.nS)\n",
    "\n",
    "    # Now, having the optimal state values when there are k-1 time steps left\n",
    "    # we can calculate the optimal state values when there are k time steps left.\n",
    "    # This is thanks to using a Dynamic Programming tabulation approach\n",
    "    for k in range(1, MAX_K):\n",
    "        # Keep a copy of V because we are going to be modifying V and we\n",
    "        # need to be taking a look at the values of the previous V\n",
    "        V_prev = np.copy(V)\n",
    "        \n",
    "        # Calculate the optimal value for each state when there\n",
    "        # are k time steps left\n",
    "        for state in range(env.nS):\n",
    "            # Calculate all the optimal Q values (action values) for the current state and time step\n",
    "            q_values = [action_value_function(state, action, V_prev, env.P, gamma) for action in range(env.nA)]\n",
    "            \n",
    "            # The value of the state in the current time step is the maximum of the Q values\n",
    "            V[state] = max(q_values)\n",
    "\n",
    "        # Break if we are done\n",
    "        if (stop_callback(k, V_prev, V)):\n",
    "            break\n",
    "            \n",
    "    return V"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def policy_extraction(env, V, gamma):\n",
    "    \"\"\"Policy extraction algorithm, it simply calculates the policy\n",
    "    given the state values and the Markov Decision Process. \n",
    "    \n",
    "    Basically this is done by looking around each state and assigning\n",
    "    as the best action the one that maximizes the expected reward. We\n",
    "    cannot say that the best action is the one which leads to the best\n",
    "    neighbor state because in a stochastic environment the action may fail\n",
    "    and end up in another state which could be the worst state (like a die state), \n",
    "    so that is why we take the action that maximizes the expected reward.\n",
    "    \n",
    "    Args:\n",
    "        env: MDP\n",
    "        V: Vector of state values\n",
    "        gamma: discount factor\n",
    "        \n",
    "    Returns:\n",
    "        Policy extracted from the given state values\n",
    "    \"\"\"\n",
    "    policy = np.zeros(env.nS, dtype=int)\n",
    "    \n",
    "    for state in range(env.nS):\n",
    "        q_values = [action_value_function(state, action, V, env.P, gamma) for action in range(env.nA)]\n",
    "        policy[state] = np.argmax(q_values)\n",
    "\n",
    "    return policy\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_episode(env, policy, render=True):\n",
    "    \"\"\"Interact with the given environment following the given policy\n",
    "    \n",
    "        Args:\n",
    "            env: MDP model of the environment\n",
    "            policy: Array acting as a map of state to actions\n",
    "            render: boolean indicating whether to call env.render() to\n",
    "                display the environment in each step or not\n",
    "                \n",
    "        Returns:\n",
    "            The total reward gotten\n",
    "    \"\"\"\n",
    "    current_env_state = env.reset()\n",
    "    done = False\n",
    "    total_reward = 0\n",
    "    \n",
    "    while not done:\n",
    "        if render:\n",
    "            env.render()\n",
    "        action = policy[current_env_state]\n",
    "        current_env_state, reward, done, _ = env.step(action)\n",
    "        total_reward += reward\n",
    "            \n",
    "    return total_reward"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\u001b[41mS\u001b[0mFFF\n",
      "FHFH\n",
      "FFFH\n",
      "HFFG\n",
      "  (Left)\n",
      "SFFF\n",
      "\u001b[41mF\u001b[0mHFH\n",
      "FFFH\n",
      "HFFG\n",
      "  (Left)\n",
      "SFFF\n",
      "\u001b[41mF\u001b[0mHFH\n",
      "FFFH\n",
      "HFFG\n",
      "  (Left)\n",
      "\u001b[41mS\u001b[0mFFF\n",
      "FHFH\n",
      "FFFH\n",
      "HFFG\n",
      "  (Left)\n",
      "\u001b[41mS\u001b[0mFFF\n",
      "FHFH\n",
      "FFFH\n",
      "HFFG\n",
      "  (Left)\n",
      "\u001b[41mS\u001b[0mFFF\n",
      "FHFH\n",
      "FFFH\n",
      "HFFG\n",
      "  (Left)\n",
      "\u001b[41mS\u001b[0mFFF\n",
      "FHFH\n",
      "FFFH\n",
      "HFFG\n",
      "  (Left)\n",
      "\u001b[41mS\u001b[0mFFF\n",
      "FHFH\n",
      "FFFH\n",
      "HFFG\n",
      "  (Left)\n",
      "SFFF\n",
      "\u001b[41mF\u001b[0mHFH\n",
      "FFFH\n",
      "HFFG\n",
      "  (Left)\n",
      "SFFF\n",
      "FHFH\n",
      "\u001b[41mF\u001b[0mFFH\n",
      "HFFG\n",
      "  (Up)\n",
      "SFFF\n",
      "FHFH\n",
      "\u001b[41mF\u001b[0mFFH\n",
      "HFFG\n",
      "  (Up)\n",
      "SFFF\n",
      "FHFH\n",
      "\u001b[41mF\u001b[0mFFH\n",
      "HFFG\n",
      "  (Up)\n",
      "SFFF\n",
      "\u001b[41mF\u001b[0mHFH\n",
      "FFFH\n",
      "HFFG\n",
      "  (Left)\n",
      "SFFF\n",
      "FHFH\n",
      "\u001b[41mF\u001b[0mFFH\n",
      "HFFG\n",
      "  (Up)\n",
      "SFFF\n",
      "FHFH\n",
      "\u001b[41mF\u001b[0mFFH\n",
      "HFFG\n",
      "  (Up)\n",
      "SFFF\n",
      "\u001b[41mF\u001b[0mHFH\n",
      "FFFH\n",
      "HFFG\n",
      "  (Left)\n",
      "SFFF\n",
      "FHFH\n",
      "\u001b[41mF\u001b[0mFFH\n",
      "HFFG\n",
      "  (Up)\n",
      "SFFF\n",
      "FHFH\n",
      "F\u001b[41mF\u001b[0mFH\n",
      "HFFG\n",
      "  (Down)\n",
      "SFFF\n",
      "FHFH\n",
      "FFFH\n",
      "H\u001b[41mF\u001b[0mFG\n",
      "  (Right)\n",
      "SFFF\n",
      "FHFH\n",
      "FFFH\n",
      "H\u001b[41mF\u001b[0mFG\n",
      "  (Right)\n",
      "SFFF\n",
      "FHFH\n",
      "F\u001b[41mF\u001b[0mFH\n",
      "HFFG\n",
      "  (Down)\n",
      "SFFF\n",
      "FHFH\n",
      "\u001b[41mF\u001b[0mFFH\n",
      "HFFG\n",
      "  (Up)\n",
      "SFFF\n",
      "FHFH\n",
      "\u001b[41mF\u001b[0mFFH\n",
      "HFFG\n",
      "  (Up)\n",
      "SFFF\n",
      "\u001b[41mF\u001b[0mHFH\n",
      "FFFH\n",
      "HFFG\n",
      "  (Left)\n",
      "SFFF\n",
      "\u001b[41mF\u001b[0mHFH\n",
      "FFFH\n",
      "HFFG\n",
      "  (Left)\n",
      "SFFF\n",
      "FHFH\n",
      "\u001b[41mF\u001b[0mFFH\n",
      "HFFG\n",
      "  (Up)\n",
      "SFFF\n",
      "FHFH\n",
      "\u001b[41mF\u001b[0mFFH\n",
      "HFFG\n",
      "  (Up)\n",
      "SFFF\n",
      "FHFH\n",
      "\u001b[41mF\u001b[0mFFH\n",
      "HFFG\n",
      "  (Up)\n",
      "SFFF\n",
      "FHFH\n",
      "\u001b[41mF\u001b[0mFFH\n",
      "HFFG\n",
      "  (Up)\n",
      "SFFF\n",
      "\u001b[41mF\u001b[0mHFH\n",
      "FFFH\n",
      "HFFG\n",
      "  (Left)\n",
      "SFFF\n",
      "\u001b[41mF\u001b[0mHFH\n",
      "FFFH\n",
      "HFFG\n",
      "  (Left)\n",
      "SFFF\n",
      "\u001b[41mF\u001b[0mHFH\n",
      "FFFH\n",
      "HFFG\n",
      "  (Left)\n",
      "\u001b[41mS\u001b[0mFFF\n",
      "FHFH\n",
      "FFFH\n",
      "HFFG\n",
      "  (Left)\n",
      "\u001b[41mS\u001b[0mFFF\n",
      "FHFH\n",
      "FFFH\n",
      "HFFG\n",
      "  (Left)\n",
      "SFFF\n",
      "\u001b[41mF\u001b[0mHFH\n",
      "FFFH\n",
      "HFFG\n",
      "  (Left)\n",
      "SFFF\n",
      "\u001b[41mF\u001b[0mHFH\n",
      "FFFH\n",
      "HFFG\n",
      "  (Left)\n",
      "SFFF\n",
      "FHFH\n",
      "\u001b[41mF\u001b[0mFFH\n",
      "HFFG\n",
      "  (Up)\n",
      "SFFF\n",
      "FHFH\n",
      "\u001b[41mF\u001b[0mFFH\n",
      "HFFG\n",
      "  (Up)\n",
      "SFFF\n",
      "\u001b[41mF\u001b[0mHFH\n",
      "FFFH\n",
      "HFFG\n",
      "  (Left)\n",
      "\u001b[41mS\u001b[0mFFF\n",
      "FHFH\n",
      "FFFH\n",
      "HFFG\n",
      "  (Left)\n",
      "SFFF\n",
      "\u001b[41mF\u001b[0mHFH\n",
      "FFFH\n",
      "HFFG\n",
      "  (Left)\n",
      "SFFF\n",
      "FHFH\n",
      "\u001b[41mF\u001b[0mFFH\n",
      "HFFG\n",
      "  (Up)\n",
      "SFFF\n",
      "FHFH\n",
      "\u001b[41mF\u001b[0mFFH\n",
      "HFFG\n",
      "  (Up)\n",
      "SFFF\n",
      "FHFH\n",
      "\u001b[41mF\u001b[0mFFH\n",
      "HFFG\n",
      "  (Up)\n",
      "SFFF\n",
      "FHFH\n",
      "F\u001b[41mF\u001b[0mFH\n",
      "HFFG\n",
      "  (Down)\n",
      "SFFF\n",
      "FHFH\n",
      "\u001b[41mF\u001b[0mFFH\n",
      "HFFG\n",
      "  (Up)\n",
      "SFFF\n",
      "\u001b[41mF\u001b[0mHFH\n",
      "FFFH\n",
      "HFFG\n",
      "  (Left)\n",
      "SFFF\n",
      "\u001b[41mF\u001b[0mHFH\n",
      "FFFH\n",
      "HFFG\n",
      "  (Left)\n",
      "SFFF\n",
      "FHFH\n",
      "\u001b[41mF\u001b[0mFFH\n",
      "HFFG\n",
      "  (Up)\n",
      "SFFF\n",
      "FHFH\n",
      "F\u001b[41mF\u001b[0mFH\n",
      "HFFG\n",
      "  (Down)\n",
      "SFFF\n",
      "FHFH\n",
      "\u001b[41mF\u001b[0mFFH\n",
      "HFFG\n",
      "  (Up)\n",
      "SFFF\n",
      "FHFH\n",
      "\u001b[41mF\u001b[0mFFH\n",
      "HFFG\n",
      "  (Up)\n",
      "SFFF\n",
      "\u001b[41mF\u001b[0mHFH\n",
      "FFFH\n",
      "HFFG\n",
      "  (Left)\n",
      "SFFF\n",
      "\u001b[41mF\u001b[0mHFH\n",
      "FFFH\n",
      "HFFG\n",
      "  (Left)\n",
      "\u001b[41mS\u001b[0mFFF\n",
      "FHFH\n",
      "FFFH\n",
      "HFFG\n",
      "  (Left)\n",
      "\u001b[41mS\u001b[0mFFF\n",
      "FHFH\n",
      "FFFH\n",
      "HFFG\n",
      "  (Left)\n",
      "\u001b[41mS\u001b[0mFFF\n",
      "FHFH\n",
      "FFFH\n",
      "HFFG\n",
      "  (Left)\n",
      "SFFF\n",
      "\u001b[41mF\u001b[0mHFH\n",
      "FFFH\n",
      "HFFG\n",
      "  (Left)\n",
      "SFFF\n",
      "FHFH\n",
      "\u001b[41mF\u001b[0mFFH\n",
      "HFFG\n",
      "  (Up)\n",
      "SFFF\n",
      "FHFH\n",
      "F\u001b[41mF\u001b[0mFH\n",
      "HFFG\n",
      "  (Down)\n",
      "SFFF\n",
      "FHFH\n",
      "FFFH\n",
      "H\u001b[41mF\u001b[0mFG\n",
      "  (Right)\n",
      "SFFF\n",
      "FHFH\n",
      "FFFH\n",
      "HF\u001b[41mF\u001b[0mG\n",
      "  (Down)\n",
      "SFFF\n",
      "FHFH\n",
      "FFFH\n",
      "HF\u001b[41mF\u001b[0mG\n",
      "Reward: 1.0\n"
     ]
    }
   ],
   "source": [
    "# MDP model of the environment\n",
    "env = gym.make('FrozenLake-v0').unwrapped\n",
    "\n",
    "# Discount factor\n",
    "gamma = 1\n",
    "\n",
    "# Callback used in value_iteration to know when to stop iterating\n",
    "# In this case we stop when the difference between the values from one time step\n",
    "#Â to the next are smaller than a threshold\n",
    "stop_evaluator = lambda k, V_prev, V: np.sum(np.fabs(V_prev - V)) <= 1E-9\n",
    "\n",
    "# Get the optimal state values via value_iteration\n",
    "V = value_iteration(env, gamma, stop_evaluator)\n",
    "\n",
    "# Get the optimal policy via policy_extraction with the optimal values\n",
    "policy = policy_extraction(env, V, gamma)\n",
    "\n",
    "# Use the optimal policy to run an episode and print also the reward\n",
    "total_reward = run_episode(env, policy)\n",
    "print(f'Reward: {total_reward}')\n",
    "\n",
    "env.close()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
