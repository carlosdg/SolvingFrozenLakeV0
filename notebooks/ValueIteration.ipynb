{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import gym"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def value_iteration(env, gamma, stop_callback):\n",
    "    \"\"\"Value iteration algorithm is used to find the optimal state values (or action values\n",
    "    but generally the state values are computed because they need less or the same memory) for \n",
    "    a Markov Decision Process where everything is known (the set of states, set of actions, reward \n",
    "    function and transition function).\n",
    "    \n",
    "    Value iteration uses a Dynamic Programming approach to find the optimal values because the\n",
    "    formula can be expressed in a recursive way and the subproblems overlap. Thanks to this we\n",
    "    have an exact method to calculate the optimal values in polynomial complexity. \n",
    "    \n",
    "    However, as many other problems, an exact method is very inefficient for very large problems.\n",
    "    This and having to know every parameter of the MDP are the weak points of the algorithms based\n",
    "    on Dynamic Programming for getting an optimal policy (or optimal values that we can use to get \n",
    "    the policy).\n",
    "    \n",
    "    Another thing to note is that this is not Reinforcement Learning, note that here we know everything\n",
    "    about the MDP and we are just applying an exact method to \"solve\" it, we are not learning anything.\n",
    "    \n",
    "    Args:\n",
    "        env: The Markov Decision Process with all parameters known\n",
    "        gamma: The discount factor.\n",
    "        stop_callback: Callback to indicate when to stop. It receives the current\n",
    "            iteration index, a numpy vector of the previous state values and a numpy\n",
    "            vector of the current state values. It must return a boolean indicating whether\n",
    "            the algorithm should stop or not (return True to stop, False to continue). This\n",
    "            way we can stop the algorithm by a maximum number of iterations or when the difference\n",
    "            between the values from one iteration to the next is so small that we can consider\n",
    "            that the algorithm converged\n",
    "\n",
    "    Returns:\n",
    "        A numpy array with the optimal state values when there are k time steps left\n",
    "        to finish the game.\n",
    "    \"\"\"\n",
    "    # Helper method:\n",
    "    # Action-value function. Returns the expected reward to get from\n",
    "    # the given state and taking the given action. V is a list of the optimal\n",
    "    # state values for the next time step\n",
    "    def action_value_function(state, action, V):\n",
    "        q_value = 0\n",
    "        for prob, next_state, reward, _ in env.P[state][action]:\n",
    "            q_value += prob * (reward + gamma * V[next_state])\n",
    "        return q_value\n",
    "    \n",
    "    # Maximum number of iterations to do\n",
    "    # So the algorithm doesn't run forever\n",
    "    MAX_K = int(1E9)\n",
    "\n",
    "    # Initialization:\n",
    "    # k=0. We have 0 time steps left, meaning that the game\n",
    "    # is finished and the agent cannot do anything\n",
    "    # so the value of every state is 0\n",
    "    # Note that we are formulating the problem this way to use a Dynamic\n",
    "    # Programming Bottom-up approach, also known as tabulation approach\n",
    "    # because we can see it as just filling a matrix\n",
    "    V = np.zeros(env.nS)\n",
    "\n",
    "    # Now, having the optimal state values when there are k-1 time steps left\n",
    "    # we can calculate the optimal state values when there are k time steps left.\n",
    "    # This thanks to using a Dynamic Programming Bottom-up approach\n",
    "    for k in range(1, MAX_K):\n",
    "        # Keep a copy of V because we are going to be modifying V and we\n",
    "        # need to be taking a look at the values of the previous V\n",
    "        V_prev = np.copy(V)\n",
    "        \n",
    "        # Calculate the optimal value for each state when there\n",
    "        # are k time steps left\n",
    "        for state in range(env.nS):\n",
    "            # Calculate all the optimal Q values (action values) for the current state and time step\n",
    "            q_values = [action_value_function(state, action, V_prev) for action in range(env.nA)]\n",
    "            \n",
    "            # The value of the state in the current time step is the maximum of the Q values\n",
    "            V[state] = max(q_values)\n",
    "\n",
    "        # Break if we are done\n",
    "        if (stop_callback(k, V_prev, V)):\n",
    "            break\n",
    "            \n",
    "    return V"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 0.82352941  0.82352941  0.82352941  0.8235294   0.82352941  0.\n",
      "  0.52941176  0.          0.82352941  0.82352941  0.76470588  0.          0.\n",
      "  0.88235294  0.94117647  0.        ]\n"
     ]
    }
   ],
   "source": [
    "env = gym.make('FrozenLake-v0').unwrapped\n",
    "\n",
    "stop_evaluator = lambda k, V_prev, V: np.sum(np.fabs(V_prev - V)) <= 1E-9\n",
    "print(value_iteration(env, 1.0, stop_evaluator))\n",
    "\n",
    "env.close()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
