{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import gym"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def action_value_function(state, action, V, transition_matrix, gamma):\n",
    "    \"\"\"(Helper method)\n",
    "    Action-value function. Returns the expected reward to get from the \n",
    "    given state and taking the given action. \n",
    "    \n",
    "    Args:\n",
    "        state: current state.\n",
    "        action: action to take from the current state.\n",
    "        V: Vector of the optimal state values for the next time step.\n",
    "        transition_matrix: Transition matrix of the MDP, each cell [s][a]\n",
    "            is a list of tuples (probability, next state, reward, done). \n",
    "            Meaning that in state s and taking action a, we can get to \n",
    "            any of those next states with the respective probability and\n",
    "            reward.\n",
    "        gamma: discount factor.\n",
    "        \n",
    "    Returns:\n",
    "        The q value (also known as action value) for the given state\n",
    "        and action\n",
    "    \"\"\"\n",
    "    q_value = 0\n",
    "    for prob, next_state, reward, _ in transition_matrix[state][action]:\n",
    "        q_value += prob * (reward + gamma * V[next_state])\n",
    "    return q_value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def value_iteration(env, gamma, stop_callback=lambda k, V_prev, V: k >= 1000):\n",
    "    \"\"\"Value iteration algorithm is used to find the optimal state values \n",
    "    (or action values but generally the state values are computed because \n",
    "    they need less or the same memory) for a Markov Decision Process where \n",
    "    everything is known (the set of states, set of actions, reward function \n",
    "    and transition function).\n",
    "    \n",
    "    Value iteration uses a Dynamic Programming approach to find the optimal \n",
    "    values because the formula can be expressed in a recursive way and the \n",
    "    subproblems overlap. Thanks to this we have an exact method to calculate \n",
    "    the optimal values in polynomial time complexity. \n",
    "    \n",
    "    However, as many other problems, an exact method is very inefficient for\n",
    "    very large problems. This and having to know every parameter of the MDP \n",
    "    are the weak points of the algorithms based on Dynamic Programming for \n",
    "    getting an optimal policy (or optimal values that we can use to get the\n",
    "    policy).\n",
    "    \n",
    "    Another thing to note is that this is not Reinforcement Learning, note \n",
    "    that here we know everything about the MDP and we are just applying an \n",
    "    exact method to \"solve\" it, we are not learning anything.\n",
    "    \n",
    "    Args:\n",
    "        env: The Markov Decision Process with all parameters known.\n",
    "        gamma: The discount factor.\n",
    "        stop_callback: Callback to indicate when to stop. It receives the current\n",
    "            iteration index, a numpy vector of the previous state values and a numpy\n",
    "            vector of the current state values. It must return a boolean indicating \n",
    "            whether the algorithm should stop or not (return True to stop, False \n",
    "            to continue). This way we can stop the algorithm by a maximum number of \n",
    "            iterations or when the difference between the values from one iteration \n",
    "            to the next is so small that we can consider that the algorithm converged.\n",
    "\n",
    "    Returns:\n",
    "        A numpy array with the optimal state values when there are k time steps left\n",
    "        to finish the game.\n",
    "    \"\"\"\n",
    "    # Maximum number of iterations to do\n",
    "    # So the algorithm doesn't run forever\n",
    "    MAX_K = int(1E9)\n",
    "\n",
    "    # Initialization:\n",
    "    # k=0. We have 0 time steps left, meaning that the game\n",
    "    # is finished and the agent cannot do anything\n",
    "    # so the value of every state is 0\n",
    "    # Note that we are formulating the problem this way to use a Dynamic\n",
    "    # Programming tabulation approach\n",
    "    V = np.zeros(env.nS)\n",
    "\n",
    "    # Now, having the optimal state values when there are k-1 time steps left\n",
    "    # we can calculate the optimal state values when there are k time steps left.\n",
    "    # This is thanks to using a Dynamic Programming tabulation approach\n",
    "    for k in range(1, MAX_K):\n",
    "        # Keep a copy of V because we are going to be modifying V and we\n",
    "        # need to be taking a look at the values of the previous V\n",
    "        V_prev = np.copy(V)\n",
    "        \n",
    "        # Calculate the optimal value for each state when there\n",
    "        # are k time steps left\n",
    "        for state in range(env.nS):\n",
    "            # Calculate all the optimal Q values (action values) \n",
    "            # for the current state and time step\n",
    "            q_values = [action_value_function(state, action, V_prev, env.P, gamma) \n",
    "                        for action in range(env.nA)]\n",
    "            \n",
    "            # The value of the state in the current time \n",
    "            # step is the  maximum of the Q values\n",
    "            V[state] = max(q_values)\n",
    "\n",
    "        # Break if we are done\n",
    "        if stop_callback(k, V_prev, V):\n",
    "            break\n",
    "            \n",
    "    return V"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def policy_extraction(env, V, gamma):\n",
    "    \"\"\"Policy extraction algorithm, it simply calculates the policy\n",
    "    given the state values and the Markov Decision Process. \n",
    "    \n",
    "    Basically this is done by looking around each state and assigning\n",
    "    as the best action the one that maximizes the expected reward. We\n",
    "    cannot say that the best action is the one which leads to the best\n",
    "    neighbor state because in a stochastic environment the action may fail\n",
    "    and end up in another state which could be the worst state (like a die \n",
    "    state), so that is why we take the action that maximizes the expected \n",
    "    reward.\n",
    "    \n",
    "    Args:\n",
    "        env: MDP\n",
    "        V: Vector of state values\n",
    "        gamma: discount factor\n",
    "        \n",
    "    Returns:\n",
    "        Policy extracted from the given state values\n",
    "    \"\"\"\n",
    "    policy = np.zeros(env.nS, dtype=int)\n",
    "    \n",
    "    for state in range(env.nS):\n",
    "        q_values = [action_value_function(state, action, V, env.P, gamma) \n",
    "                    for action in range(env.nA)]\n",
    "        policy[state] = np.argmax(q_values)\n",
    "\n",
    "    return policy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def policy_evaluation(env, policy, gamma, stop_callback=lambda k, V_prev, V: k >= 1000):\n",
    "    \"\"\"Policy evaluation algorithm, it simply calculates the state \n",
    "    values for the given MDP and policy. \n",
    "    \n",
    "    This is needed so we can compare two policies. By just looking\n",
    "    at two policies we cannot say which one is better, they are only\n",
    "    a map of states to actions. But, by having the state values we\n",
    "    can say that a policy is better than another one if all the state\n",
    "    values are better\n",
    "    \n",
    "    Args:\n",
    "        env: The Markov Decision Process with all parameters known.\n",
    "        policy: Map from states to actions\n",
    "        gamma: The discount factor.\n",
    "        stop_callback: Callback to indicate when to stop. It receives the current\n",
    "            iteration index, a numpy vector of the previous state values and a numpy\n",
    "            vector of the current state values. It must return a boolean indicating \n",
    "            whether the algorithm should stop or not (return True to stop, False \n",
    "            to continue). This way we can stop the algorithm by a maximum number of \n",
    "            iterations or when the difference between the values from one iteration \n",
    "            to the next is so small that we can consider that the algorithm converged.\n",
    "\n",
    "    Returns:\n",
    "        A numpy array with the state values for the given MDP and policy\n",
    "    \"\"\"\n",
    "    MAX_K = int(1E9)\n",
    "    V = np.zeros(env.nS)\n",
    "    \n",
    "    for k in range(1, MAX_K):\n",
    "        V_prev = np.copy(V)\n",
    "        for state in range(env.nS):\n",
    "            # This algorithm looks a lot like value iteration. However\n",
    "            # note that unlike in value iteration, here we don't want\n",
    "            # to find the action that maximizes the value. The action\n",
    "            # to take is fixed by the policy that we are evaluating\n",
    "            action = policy[state]\n",
    "            V[state] = action_value_function(state, action, V_prev, env.P, gamma)\n",
    "            \n",
    "        if stop_callback(k, V_prev, V):\n",
    "            break\n",
    "            \n",
    "    return V"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def policy_iteration(env, gamma):\n",
    "    \"\"\"Policy iteration algorithm. This is an alternative algorithm\n",
    "    to value iteration. An observation was made that the policy tends\n",
    "    to converge much quicker than the values, so this algorithm tries\n",
    "    to exploit that observation.\n",
    "    \n",
    "    First we start with a random policy, then, iteratively run policy\n",
    "    evaluation to get the state values and policy extraction to get\n",
    "    a policy from the recent state values. \n",
    "    \n",
    "    (Intuitive explanation:)\n",
    "    This works because when policy extraction runs it doesn't know \n",
    "    about the policy, it just see the state values and try to get\n",
    "    the policy by assigning the BEST action, so an action for a state\n",
    "    in this policy can be different (improved) from the previous policy.\n",
    "    And when the policy evaluation runs, if the policy is different\n",
    "    (improved) than the previous one, then the values will be \n",
    "    different (better).\n",
    "    \n",
    "    Policy iteration is also an algorithm based on dynamic programming\n",
    "    and needs to know everything about the MDP so its weak points\n",
    "    are the same as with value iteration.\n",
    "    \n",
    "    Args:\n",
    "        env:\n",
    "        gamma:\n",
    "        \n",
    "    Returns:\n",
    "        The best map from state to actions (policy) to follow if you\n",
    "        want to maximize the accumulated rewards in a MDP\n",
    "    \"\"\"    \n",
    "    # Callback used to tell policy evaluation when to stop (when\n",
    "    # the difference between the previous V and the current is less\n",
    "    # than a threshold)\n",
    "    def policy_evaluation_stop_callback(k, V_prev, V):\n",
    "        return np.sum(np.fabs(V_prev - V)) <= 1E-6\n",
    "    \n",
    "    # Random initialization for the policy\n",
    "    policy = np.random.randint(low=0, high=env.nA, size=env.nS)\n",
    "    \n",
    "    while True:\n",
    "        # Evaluation step\n",
    "        V = policy_evaluation(env, policy, gamma, policy_evaluation_stop_callback)\n",
    "        \n",
    "        # Improvement step\n",
    "        new_policy = policy_extraction(env, V, gamma)\n",
    "        \n",
    "        # Stop when the algorithm converges (the policy\n",
    "        # doesn't update anymore)\n",
    "        if np.array_equal(policy, new_policy):\n",
    "            break\n",
    "        else:\n",
    "            policy = new_policy\n",
    "    \n",
    "    return policy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_episode(env, policy, render=True):\n",
    "    \"\"\"Interact with the given environment following the given policy\n",
    "    \n",
    "        Args:\n",
    "            env: MDP model of the environment\n",
    "            policy: Array acting as a map of state to actions\n",
    "            render: boolean indicating whether to call env.render() to\n",
    "                display the environment in each step or not\n",
    "                \n",
    "        Returns:\n",
    "            The total reward gotten\n",
    "    \"\"\"\n",
    "    current_env_state = env.reset()\n",
    "    done = False\n",
    "    total_reward = 0\n",
    "    \n",
    "    while not done:\n",
    "        if render:\n",
    "            env.render()\n",
    "        action = policy[current_env_state]\n",
    "        current_env_state, reward, done, _ = env.step(action)\n",
    "        total_reward += reward\n",
    "            \n",
    "    return total_reward"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ~~~~~ MAIN ~~~~~\n",
    "# Definition of the model parameters\n",
    "\n",
    "# MDP model of the environment\n",
    "env = gym.make('FrozenLake-v0').unwrapped\n",
    "\n",
    "# Discount factor\n",
    "gamma = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 90 ms, sys: 0 ns, total: 90 ms\n",
      "Wall time: 76.7 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "# Run value iteration and policy extraction to get the optimal policy\n",
    "\n",
    "# Callback used in value_iteration to know when to stop iterating\n",
    "# In this case we stop when the difference between the values from \n",
    "# one time step to the next are smaller than a threshold\n",
    "def value_iteration_stop_callback(k, V_prev, V):\n",
    "    return np.sum(np.fabs(V_prev - V)) <= 1E-6\n",
    "\n",
    "# Get the optimal state values via value_iteration\n",
    "V = value_iteration(env, gamma, value_iteration_stop_callback)\n",
    "\n",
    "# Get the optimal policy via policy_extraction with the optimal values\n",
    "policy_from_value_iteration = policy_extraction(env, V, gamma)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 60 ms, sys: 0 ns, total: 60 ms\n",
      "Wall time: 62.5 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "# Run policy iteration to get the optimal policy\n",
    "policy_from_policy_iteration = policy_iteration(env, gamma)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Policy from value iteration:   [0 3 3 3 0 0 0 0 3 1 0 0 0 2 1 0]\n",
      "Policy from policy iteration:  [0 3 3 3 0 0 0 0 3 1 0 0 0 2 1 0]\n",
      "Reward: 1.0\n"
     ]
    }
   ],
   "source": [
    "# Show both policies and run an episode with one of them\n",
    "print(\"Policy from value iteration:  \", policy_from_value_iteration)\n",
    "print(\"Policy from policy iteration: \", policy_from_policy_iteration)\n",
    "\n",
    "total_reward = run_episode(env, policy_from_policy_iteration, render=False)\n",
    "print(f'Reward: {total_reward}')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
